#!/usr/bin/env python3
"""
Enhanced Embeddings Uploader - With Exact Tile Bounds
====================================================

This enhanced version extracts exact tile boundary coordinates from your
GeoParquet files and includes them in the migration to both Qdrant and metadata files.
"""

import os
import json
import numpy as np
import pandas as pd
import geopandas as gpd
from glob import glob
from typing import Dict, List, Set, Tuple, Optional
import umap
from qdrant_client import QdrantClient
from qdrant_client.models import Distance, VectorParams, PointStruct
from qdrant_client.http import models
import logging
from tqdm import tqdm
import time
from dotenv import load_dotenv

# Load environment variables
load_dotenv()

# Configuration
FORCE_REUPLOAD = False  # Set to False for incremental uploads
EMBEDDINGS_DIR = "/Users/janmagnuszewski/dev/terramind/embeddings/urban_embeddings_224_terramind"
OUTPUT_DIR = "./production_data"

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class EnhancedEmbeddingsUploader:
    def __init__(self):
        """Initialize the enhanced uploader with exact bounds extraction."""
        # Qdrant connection
        self.qdrant_url = os.getenv('QDRANT_URL', 'http://localhost:6333')
        self.qdrant_api_key = os.getenv('QDRANT_API_KEY')
        
        # Connect to Qdrant
        try:
            client_kwargs = {'url': self.qdrant_url, 'timeout': 120}
            if self.qdrant_api_key and self.qdrant_api_key != 'your_api_key_if_needed':
                client_kwargs['api_key'] = self.qdrant_api_key
                
            self.qdrant_client = QdrantClient(**client_kwargs)
            logger.info(f"âœ… Connected to Qdrant at {self.qdrant_url}")
        except Exception as e:
            logger.error(f"âŒ Failed to connect to Qdrant: {e}")
            raise
        
        # Create output directory
        os.makedirs(OUTPUT_DIR, exist_ok=True)
        
        # Collection names
        self.collections = {
            'regular': 'satellite_embeddings_simple',
            'global_contrastive': 'satellite_embeddings_global_contrastive_simple'
        }
        
        # Track existing cities
        self.existing_cities = set()
        if not FORCE_REUPLOAD:
            self.existing_cities = self._load_existing_cities()
    
    def _load_existing_cities(self) -> Set[str]:
        """Load existing cities from regular collection."""
        existing_cities = set()
        collection_name = self.collections['regular']
        
        try:
            logger.info(f"ğŸ” Loading existing cities from {collection_name}...")
            
            offset = None
            while True:
                points, next_offset = self.qdrant_client.scroll(
                    collection_name=collection_name,
                    limit=1000,
                    offset=offset,
                    with_payload=True,
                    with_vectors=False
                )
                
                for point in points:
                    if hasattr(point, 'payload') and point.payload:
                        city = point.payload.get('city')
                        country = point.payload.get('country')
                        if city and country:
                            city_key = f"{city}, {country}"
                            existing_cities.add(city_key)
                
                if next_offset is None:
                    break
                offset = next_offset
            
            logger.info(f"ğŸ“Š Found {len(existing_cities)} existing cities")
            
        except Exception as e:
            logger.warning(f"âš ï¸ Could not load existing cities: {e}")
        
        return existing_cities
    
    def extract_exact_tile_bounds(self, row: pd.Series) -> Optional[List[List[float]]]:
        """
        Extract exact tile boundary coordinates from GeoParquet geometry.
        
        Args:
            row: DataFrame row containing tile data
            
        Returns:
            List of [lon, lat] coordinates defining the tile boundary, or None if not available
        """
        try:
            # Method 1: Check if we have explicit bounds columns from the embedding script
            bounds_cols = ['bounds_min_lon', 'bounds_min_lat', 'bounds_max_lon', 'bounds_max_lat']
            if all(col in row.index for col in bounds_cols):
                min_lon = float(row['bounds_min_lon'])
                min_lat = float(row['bounds_min_lat'])
                max_lon = float(row['bounds_max_lon'])
                max_lat = float(row['bounds_max_lat'])
                
                # Create tile boundary as polygon coordinates [lon, lat]
                tile_bounds = [
                    [min_lon, max_lat],  # Top-left
                    [max_lon, max_lat],  # Top-right
                    [max_lon, min_lat],  # Bottom-right
                    [min_lon, min_lat],  # Bottom-left
                    [min_lon, max_lat]   # Close polygon
                ]
                
                return tile_bounds
            
            # Method 2: Try to extract from geometry column if it's a polygon
            elif 'geometry' in row.index and row['geometry'] is not None:
                geom = row['geometry']
                if hasattr(geom, 'bounds'):
                    min_lon, min_lat, max_lon, max_lat = geom.bounds
                    tile_bounds = [
                        [min_lon, max_lat],
                        [max_lon, max_lat],
                        [max_lon, min_lat],
                        [min_lon, min_lat],
                        [min_lon, max_lat]
                    ]
                    return tile_bounds
                elif hasattr(geom, 'exterior') and hasattr(geom.exterior, 'coords'):
                    # Extract coordinates directly from polygon exterior
                    coords = list(geom.exterior.coords)
                    # Convert from (x,y) to [lon,lat] format
                    tile_bounds = [[coord[0], coord[1]] for coord in coords]
                    return tile_bounds
            
            return None
            
        except Exception as e:
            logger.debug(f"Could not extract exact bounds: {e}")
            return None
    
    def delete_all_collections(self):
        """Delete all collections if they exist."""
        logger.info("ğŸ—‘ï¸ Deleting all collections...")
        
        try:
            collections = self.qdrant_client.get_collections().collections
            existing_names = [col.name for col in collections]
            
            for embed_type, collection_name in self.collections.items():
                if collection_name in existing_names:
                    logger.info(f"ğŸ—‘ï¸ Deleting {collection_name}")
                    self.qdrant_client.delete_collection(collection_name)
                    logger.info(f"âœ… Deleted {collection_name}")
                else:
                    logger.info(f"â„¹ï¸ Collection {collection_name} doesn't exist")
            
            time.sleep(2)  # Wait for deletion to complete
            logger.info("âœ… All collections deleted")
            
        except Exception as e:
            logger.error(f"âŒ Error deleting collections: {e}")
            raise
    
    def create_collections(self):
        """Create fresh collections."""
        logger.info("ğŸ“¦ Creating collections...")
        
        for embed_type, collection_name in self.collections.items():
            try:
                logger.info(f"ğŸ“¦ Creating {collection_name}")
                
                self.qdrant_client.create_collection(
                    collection_name=collection_name,
                    vectors_config=VectorParams(
                        size=768,
                        distance=Distance.COSINE,
                        on_disk=True
                    ),
                    hnsw_config=models.HnswConfigDiff(
                        m=16,
                        ef_construct=100
                    )
                )
                
                logger.info(f"âœ… Created {collection_name}")
                
            except Exception as e:
                logger.error(f"âŒ Failed to create {collection_name}: {e}")
                raise
        
        logger.info("âœ… All collections created")
    
    def process_tile_enhanced(self, row: pd.Series) -> Tuple[Optional[Dict], Optional[np.ndarray]]:
        """Enhanced tile processing that includes exact tile bounds."""
        try:
            # Get coordinates
            longitude = row.get('centroid_lon') or row.get('longitude')
            latitude = row.get('centroid_lat') or row.get('latitude')
            
            if longitude is None or latitude is None:
                return None, None
            
            if pd.isna(longitude) or pd.isna(latitude):
                return None, None
            
            lon_val = float(longitude)
            lat_val = float(latitude)
            
            if lon_val < -180 or lon_val > 180 or lat_val < -90 or lat_val > 90:
                return None, None
            
            # Get embedding
            full_embedding = row.get('embedding_patches_full')
            aggregated_embedding_col = row.get('embedding_patch_aggregated')
            
            # Process embeddings (same logic as before)
            if full_embedding is not None:
                try:
                    if isinstance(full_embedding, np.ndarray):
                        embedding_array = full_embedding
                    else:
                        embedding_array = np.array(full_embedding)
                    
                    if embedding_array.size == 196 * 768:
                        patches = embedding_array.reshape(196, 768)
                        aggregated_embedding = patches.mean(axis=0)
                    else:
                        logger.warning(f"Unexpected full embedding size: {embedding_array.size}")
                        return None, None
                        
                except Exception as e:
                    logger.warning(f"Error processing full patch embedding: {e}")
                    return None, None
            
            elif aggregated_embedding_col is not None:
                try:
                    if isinstance(aggregated_embedding_col, np.ndarray):
                        aggregated_embedding = aggregated_embedding_col
                    else:
                        aggregated_embedding = np.array(aggregated_embedding_col)
                    
                    if aggregated_embedding.size != 768:
                        logger.warning(f"Unexpected aggregated embedding size: {aggregated_embedding.size}")
                        return None, None
                        
                except Exception as e:
                    logger.warning(f"Error processing aggregated embedding: {e}")
                    return None, None
            else:
                return None, None
            
            # Generate tile ID
            tile_id = row.get('tile_id')
            if tile_id is None or pd.isna(tile_id):
                tile_id = f"tile_{lon_val:.6f}_{lat_val:.6f}".replace('.', '_').replace('-', 'neg')
            
            unique_id = abs(hash(str(tile_id))) % (2**31 - 1)
            
            # Get required fields
            city = row.get('city')
            country = row.get('country')
            continent = row.get('continent', 'Unknown')
            
            def is_valid_string_field(val):
                if val is None:
                    return False
                try:
                    if pd.isna(val):
                        return False
                except (ValueError, TypeError):
                    pass
                str_val = str(val).strip().lower()
                return str_val and str_val not in ['unknown', 'nan', 'none', '']
            
            if not is_valid_string_field(city) or not is_valid_string_field(country):
                return None, None
            
            if not is_valid_string_field(continent):
                continent = 'Unknown'
            
            # Get date
            date_str = None
            acq_date = row.get('acquisition_date')
            if acq_date is not None:
                try:
                    if not pd.isna(acq_date):
                        date_str = str(acq_date)
                except (ValueError, TypeError):
                    date_str = str(acq_date) if str(acq_date) != 'nan' else None
            
            # **ENHANCED: Extract exact tile bounds**
            exact_tile_bounds = self.extract_exact_tile_bounds(row)
            
            # Create enhanced metadata with exact bounds
            metadata = {
                'id': unique_id,
                'original_tile_id': str(tile_id),
                'city': str(city).strip(),
                'country': str(country).strip(),
                'continent': str(continent).strip(),
                'longitude': lon_val,
                'latitude': lat_val,
                'date': date_str,
                # **ENHANCED: Add exact tile bounds if available**
                'tile_bounds': exact_tile_bounds,
                'has_exact_bounds': exact_tile_bounds is not None
            }
            
            # Add tile size information if we have bounds
            if exact_tile_bounds:
                # Calculate tile dimensions
                lons = [coord[0] for coord in exact_tile_bounds[:-1]]  # Exclude closing point
                lats = [coord[1] for coord in exact_tile_bounds[:-1]]
                
                tile_width_deg = max(lons) - min(lons)
                tile_height_deg = max(lats) - min(lats)
                
                metadata.update({
                    'tile_width_degrees': tile_width_deg,
                    'tile_height_degrees': tile_height_deg
                })
            
            return metadata, aggregated_embedding
            
        except Exception as e:
            logger.warning(f"Error processing tile: {e}")
            return None, None
    
    def load_all_embeddings_enhanced(self) -> Tuple[List[Dict], np.ndarray]:
        """Enhanced embedding loading with exact tile bounds extraction."""
        logger.info("ğŸ“– Loading embeddings with exact tile bounds...")
        
        # Find all parquet files
        pattern = os.path.join(EMBEDDINGS_DIR, "full_patch_embeddings", "*", "*.gpq")
        files = glob(pattern)
        
        if not files:
            raise ValueError(f"No .gpq files found in {pattern}")
        
        logger.info(f"ğŸ“„ Found {len(files)} files to process")
        
        all_metadata = []
        all_embeddings = []
        processed_cities = set()
        skipped_cities = set()
        tiles_with_exact_bounds = 0
        tiles_with_fallback_bounds = 0
        
        for file_path in tqdm(files, desc="Processing files with bounds extraction"):
            try:
                # Extract city info from filename
                country_dir = os.path.basename(os.path.dirname(file_path))
                city_file = os.path.basename(file_path).replace('.gpq', '')
                
                # Load the GeoParquet file
                gdf = gpd.read_parquet(file_path)
                
                if len(gdf) == 0:
                    logger.warning(f"âš ï¸ Empty file: {file_path}")
                    continue
                
                # Get city info from first row
                first_row = gdf.iloc[0]
                city_name = first_row.get('city', '').strip()
                country_name = first_row.get('country', '').strip()
                
                if not city_name or not country_name:
                    logger.warning(f"âš ï¸ Missing city/country info in {file_path}")
                    continue
                
                city_key = f"{city_name}, {country_name}"
                
                # City-level check for incremental updates
                if not FORCE_REUPLOAD and city_key in self.existing_cities:
                    logger.info(f"â­ï¸ Skipping {city_key} - already exists")
                    skipped_cities.add(city_key)
                    continue
                
                logger.info(f"ğŸ™ï¸ Processing {city_key} - {len(gdf)} tiles")
                
                # Convert categorical columns
                for col in gdf.columns:
                    if gdf[col].dtype.name == 'category':
                        gdf[col] = gdf[col].astype(str)
                
                city_tile_count = 0
                city_exact_bounds = 0
                
                for _, row in gdf.iterrows():
                    metadata, embedding = self.process_tile_enhanced(row)
                    
                    if metadata and embedding is not None:
                        all_metadata.append(metadata)
                        all_embeddings.append(embedding)
                        city_tile_count += 1
                        
                        # Track bounds statistics
                        if metadata.get('has_exact_bounds', False):
                            city_exact_bounds += 1
                            tiles_with_exact_bounds += 1
                        else:
                            tiles_with_fallback_bounds += 1
                
                if city_tile_count > 0:
                    processed_cities.add(city_key)
                    bounds_info = f"({city_exact_bounds} exact bounds)" if city_exact_bounds > 0 else "(fallback bounds)"
                    logger.info(f"âœ… {city_key}: {city_tile_count} tiles {bounds_info}")
                else:
                    logger.warning(f"âš ï¸ {city_key}: No valid tiles found")
                
            except Exception as e:
                logger.error(f"âŒ Error processing {file_path}: {e}")
                continue
        
        if not all_embeddings:
            logger.warning("âš ï¸ No tiles to upload!")
            return [], np.array([])
        
        # Enhanced summary with bounds statistics
        logger.info(f"ğŸ“Š Enhanced Processing Summary:")
        logger.info(f"   New cities processed: {len(processed_cities)}")
        logger.info(f"   Cities skipped: {len(skipped_cities)}")
        logger.info(f"   Total tiles to upload: {len(all_embeddings)}")
        logger.info(f"   ğŸ¯ Tiles with exact bounds: {tiles_with_exact_bounds}")
        logger.info(f"   ğŸ“ Tiles with fallback bounds: {tiles_with_fallback_bounds}")
        logger.info(f"   ğŸ“Š Exact bounds coverage: {tiles_with_exact_bounds/len(all_embeddings)*100:.1f}%")
        
        return all_metadata, np.array(all_embeddings)
    
    def upload_to_qdrant(self, metadata_list: List[Dict], regular_embeddings: np.ndarray):
        """Upload embeddings to Qdrant collections."""
        if len(metadata_list) == 0:
            logger.info("ğŸ“ No tiles to upload")
            return
        
        # Compute dataset mean for global contrastive
        logger.info("ğŸ§® Computing dataset mean for global contrastive embeddings...")
        dataset_mean = regular_embeddings.mean(axis=0)
        global_contrastive = regular_embeddings - dataset_mean
        
        embeddings_dict = {
            'regular': regular_embeddings,
            'global_contrastive': global_contrastive
        }
        
        # Upload to each collection
        batch_size = 100
        
        for embed_type, embeddings_array in embeddings_dict.items():
            collection_name = self.collections[embed_type]
            logger.info(f"ğŸš€ Uploading {len(metadata_list)} {embed_type} embeddings to {collection_name}")
            
            total_batches = (len(metadata_list) + batch_size - 1) // batch_size
            
            for i in tqdm(range(0, len(metadata_list), batch_size), 
                         desc=f"Uploading {embed_type}", total=total_batches):
                
                batch_metadata = metadata_list[i:i + batch_size]
                batch_embeddings = embeddings_array[i:i + batch_size]
                
                # Create points
                points = []
                for metadata, embedding in zip(batch_metadata, batch_embeddings):
                    payload = {
                        'city': metadata['city'],
                        'country': metadata['country'],
                        'continent': metadata['continent'],
                        'longitude': metadata['longitude'],
                        'latitude': metadata['latitude'],
                        'date': metadata['date'],
                        'original_tile_id': metadata['original_tile_id'],
                        'embedding_type': embed_type,
                        'city_name': f"{metadata['city']}, {metadata['country']}",
                        # **ENHANCED: Include bounds information in Qdrant payload**
                        'has_exact_bounds': metadata.get('has_exact_bounds', False)
                    }
                    
                    # Remove None values
                    payload = {k: v for k, v in payload.items() if v is not None}
                    
                    point = PointStruct(
                        id=metadata['id'],
                        vector=embedding.tolist(),
                        payload=payload
                    )
                    points.append(point)
                
                # Upload batch
                try:
                    operation_info = self.qdrant_client.upsert(
                        collection_name=collection_name,
                        wait=True,
                        points=points
                    )
                    
                    if operation_info.status != models.UpdateStatus.COMPLETED:
                        logger.warning(f"âš ï¸ Batch upload status: {operation_info.status}")
                
                except Exception as e:
                    logger.error(f"âŒ Error uploading batch: {e}")
                    continue
            
            logger.info(f"âœ… {embed_type} upload completed")
        
        # Save enhanced metadata
        self.save_enhanced_data(metadata_list, regular_embeddings, dataset_mean)
    
    def save_enhanced_data(self, metadata_list: List[Dict], regular_embeddings: np.ndarray, dataset_mean: np.ndarray):
        """Save metadata with exact tile bounds information."""
        
        if FORCE_REUPLOAD:
            logger.info("ğŸ’¾ Computing UMAP and saving enhanced data with exact bounds...")
            
            # Compute UMAP
            n_samples = len(regular_embeddings)
            n_neighbors = min(50, max(5, n_samples // 100))
            
            logger.info(f"ğŸ—ºï¸ Computing UMAP with n_neighbors={n_neighbors}")
            
            reducer = umap.UMAP(
                n_components=2,
                n_neighbors=n_neighbors,
                min_dist=0.3,
                metric='cosine',
                random_state=42
            )
            
            umap_coords = reducer.fit_transform(regular_embeddings)
            
            # Enhanced UMAP data with bounds information
            umap_data = {
                'umap_points': [
                    {
                        'location_id': metadata['id'],
                        'x': float(umap_coords[i][0]),
                        'y': float(umap_coords[i][1]),
                        'city': metadata['city'],
                        'country': metadata['country'],
                        'continent': metadata['continent'],
                        'longitude': metadata['longitude'],
                        'latitude': metadata['latitude'],
                        'date': metadata['date'],
                        # **ENHANCED: Include bounds information in UMAP data**
                        'tile_bounds': metadata.get('tile_bounds'),
                        'has_exact_bounds': metadata.get('has_exact_bounds', False)
                    }
                    for i, metadata in enumerate(metadata_list)
                ],
                'total_points': len(metadata_list),
                'bounds_statistics': {
                    'tiles_with_exact_bounds': sum(1 for m in metadata_list if m.get('has_exact_bounds', False)),
                    'tiles_with_fallback_bounds': sum(1 for m in metadata_list if not m.get('has_exact_bounds', False)),
                    'exact_bounds_percentage': sum(1 for m in metadata_list if m.get('has_exact_bounds', False)) / len(metadata_list) * 100
                }
            }
            
            dataset_stats = {
                'total_samples': len(metadata_list),
                'embedding_dimension': 768,
                'dataset_mean': dataset_mean.tolist(),
                'collections': self.collections,
                'force_reupload': FORCE_REUPLOAD,
                'timestamp': time.time(),
                'enhanced_features': {
                    'exact_tile_bounds': True,
                    'bounds_coverage_percentage': umap_data['bounds_statistics']['exact_bounds_percentage']
                }
            }
            
            # Save enhanced files
            files_to_save = [
                ('locations_metadata.json', metadata_list),
                ('umap_coordinates.json', umap_data),
                ('dataset_statistics.json', dataset_stats)
            ]
            
            for filename, data in files_to_save:
                filepath = os.path.join(OUTPUT_DIR, filename)
                with open(filepath, 'w') as f:
                    json.dump(data, f, indent=2)
                logger.info(f"ğŸ“„ Saved enhanced {filename}")
            
            # Log bounds statistics
            bounds_stats = umap_data['bounds_statistics']
            logger.info(f"ğŸ¯ Tile Bounds Statistics:")
            logger.info(f"   Exact bounds: {bounds_stats['tiles_with_exact_bounds']}")
            logger.info(f"   Fallback bounds: {bounds_stats['tiles_with_fallback_bounds']}")
            logger.info(f"   Coverage: {bounds_stats['exact_bounds_percentage']:.1f}%")
        
        else:
            # Incremental update logic
            logger.info("ğŸ’¾ Updating existing metadata with exact bounds...")
            
            # Load existing metadata
            metadata_file = os.path.join(OUTPUT_DIR, 'locations_metadata.json')
            existing_metadata = []
            if os.path.exists(metadata_file):
                with open(metadata_file, 'r') as f:
                    existing_metadata = json.load(f)
                logger.info(f"ğŸ“„ Loaded {len(existing_metadata)} existing metadata records")
            
            # Merge new metadata (avoid duplicates)
            existing_ids = {item['id'] for item in existing_metadata}
            new_count = 0
            for metadata in metadata_list:
                if metadata['id'] not in existing_ids:
                    existing_metadata.append(metadata)
                    new_count += 1
            
            logger.info(f"ğŸ“„ Added {new_count} new metadata records")
            
            # Update metadata file
            with open(metadata_file, 'w') as f:
                json.dump(existing_metadata, f, indent=2)
            logger.info(f"ğŸ“„ Updated locations_metadata.json with {len(existing_metadata)} total records")
            
            # Calculate enhanced statistics
            exact_bounds_count = sum(1 for m in existing_metadata if m.get('has_exact_bounds', False))
            fallback_bounds_count = len(existing_metadata) - exact_bounds_count
            exact_bounds_percentage = (exact_bounds_count / len(existing_metadata)) * 100 if existing_metadata else 0
            
            # Update dataset stats
            stats_file = os.path.join(OUTPUT_DIR, 'dataset_statistics.json')
            dataset_stats = {
                'total_samples': len(existing_metadata),
                'embedding_dimension': 768,
                'dataset_mean': dataset_mean.tolist(),
                'collections': self.collections,
                'force_reupload': FORCE_REUPLOAD,
                'timestamp': time.time(),
                'new_tiles_added': new_count,
                'enhanced_features': {
                    'exact_tile_bounds': True,
                    'bounds_coverage_percentage': exact_bounds_percentage
                }
            }
            
            with open(stats_file, 'w') as f:
                json.dump(dataset_stats, f, indent=2)
            logger.info(f"ğŸ“„ Updated dataset_statistics.json")
            
            # Log enhanced bounds statistics
            logger.info(f"ğŸ¯ Enhanced Bounds Coverage:")
            logger.info(f"   Exact bounds: {exact_bounds_count}")
            logger.info(f"   Fallback bounds: {fallback_bounds_count}")
            logger.info(f"   Coverage: {exact_bounds_percentage:.1f}%")
            
            # Note: UMAP coordinates are NOT recomputed in incremental mode
            logger.info("â„¹ï¸ UMAP coordinates NOT recomputed in incremental mode")
            logger.info("â„¹ï¸ To get UMAP for new tiles, run with FORCE_REUPLOAD=True")
        
        logger.info(f"ğŸ’¾ Enhanced data saved to {OUTPUT_DIR}")
    
    def run(self):
        """Main execution method with enhanced bounds processing."""
        start_time = time.time()
        
        if FORCE_REUPLOAD:
            logger.info("ğŸ”„ FORCE REUPLOAD MODE: Enhanced processing with exact bounds")
            self.delete_all_collections()
            self.create_collections()
        else:
            logger.info("â• INCREMENTAL MODE: Enhanced processing with exact bounds")
            # Check if collections exist, create if not
            try:
                collections = self.qdrant_client.get_collections().collections
                existing_names = [col.name for col in collections]
                
                for embed_type, collection_name in self.collections.items():
                    if collection_name not in existing_names:
                        logger.info(f"ğŸ“¦ Creating missing collection: {collection_name}")
                        self.qdrant_client.create_collection(
                            collection_name=collection_name,
                            vectors_config=VectorParams(size=768, distance=Distance.COSINE, on_disk=True)
                        )
            except Exception as e:
                logger.error(f"âŒ Error checking/creating collections: {e}")
                raise
        
        # Load embeddings with exact bounds
        metadata_list, regular_embeddings = self.load_all_embeddings_enhanced()
        
        if len(metadata_list) > 0:
            self.upload_to_qdrant(metadata_list, regular_embeddings)
            
            # Enhanced summary
            duration = (time.time() - start_time) / 60
            exact_bounds_count = sum(1 for m in metadata_list if m.get('has_exact_bounds', False))
            
            logger.info(f"\nğŸ‰ Enhanced upload completed!")
            logger.info(f"â±ï¸ Duration: {duration:.1f} minutes")
            logger.info(f"ğŸ“Š Tiles uploaded: {len(metadata_list)}")
            logger.info(f"ğŸ¯ Tiles with exact bounds: {exact_bounds_count}")
            logger.info(f"ğŸ“ Exact bounds coverage: {exact_bounds_count/len(metadata_list)*100:.1f}%")
        else:
            logger.info("âœ… No new tiles to upload")


def main():
    """Main function with enhanced bounds processing."""
    logger.info("ğŸ›°ï¸ Enhanced Embeddings Uploader - With Exact Tile Bounds")
    logger.info("=" * 70)
    logger.info(f"ğŸ“ Embeddings directory: {EMBEDDINGS_DIR}")
    logger.info(f"ğŸ“ Output directory: {OUTPUT_DIR}")
    logger.info(f"ğŸ”„ Force reupload: {FORCE_REUPLOAD}")
    
    # Show the collection names that will be created
    collections = {
        'regular': 'satellite_embeddings_simple',
        'global_contrastive': 'satellite_embeddings_global_contrastive_simple'
    }
    logger.info(f"ğŸ“¦ Collections to create/use:")
    for method, collection_name in collections.items():
        logger.info(f"   - {method}: {collection_name}")
    
    if not os.path.exists(EMBEDDINGS_DIR):
        logger.error(f"âŒ Embeddings directory not found: {EMBEDDINGS_DIR}")
        return 1
    
    uploader = EnhancedEmbeddingsUploader()
    
    try:
        uploader.run()
        logger.info("ğŸ‰ Enhanced script completed successfully!")
        logger.info("ğŸ¯ Exact tile bounds have been extracted and included in the migration!")
        logger.info("ğŸ“ Update your frontend and backend to use the enhanced models and MapView")
        return 0
    except Exception as e:
        logger.error(f"âŒ Enhanced script failed: {e}")
        import traceback
        logger.error(traceback.format_exc())
        return 1


if __name__ == "__main__":
    exit(main())